{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam pipelines\n",
    "\n",
    "Un \"program\" è composto da:\n",
    "\n",
    "* driver: la pipeline (da sorgenti dati in input a risultati in output)\n",
    "* runner: esecutore del driver (locale, gcp dataflow, flink, spark, ...)\n",
    "\n",
    "Funziona sia con dati batch sia con dati in streaming: entrambi condividono le stesse astrazioni:\n",
    "\n",
    "* **Pipeline:** incapsula tutto il data processing task dall'inizio alla fine.\n",
    "* **PCollection:** sono gli input e gli output di ogni stadio della pipeline. Sono dataset distribuiti su cui opera la pipeline, *bounded* (data source fissa/batch, ie un file) o *unbounded* (data source continua/stream, ie kafka o pub/sub topic o subscriptions in generale). Si generano o leggendo sorgenti dati o in memory a seguito dei processing steps della pipeline.\n",
    "* **PTransform:** sono le operazioni di data processing fatte sulle PCollections, aka gli step computazionali della pipeline. Da 1+ PCollections in input, generano 0+ PCollections in output.\n",
    "* **IO Transform:** lettura/scrittura di dati su vari sistemi di storage (cloud, file system locale, ...)\n",
    "\n",
    "Di seguito un esempip di un tipico WordCount tramite Beam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Options\n",
    "\n",
    "Stabilire le pipeline options, fra cui input/output storage e quale runner (locale/distribuito) usare.\n",
    "\n",
    "Tipicamente si lasciano specificati da terminale, si parsano con argparse ed eventualmente si mettono a default.\n",
    "\n",
    "Qui non ci sono perchè per eseguire in locale non servono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_options = PipelineOptions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computazioni pesanti\n",
    "\n",
    "Andranno messe dentro i **ParDo**, che verranno eseguiti in parallelo su worker diversi al fine di ridurre l'overhead computazionale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeCleanLineFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        \"\"\"\n",
    "        Splits a text line into a list of tuples (word, next_word)\n",
    "        element: a text line, typed as string e.g. \"Hi, I am Gabriele!\"\n",
    "        return: a list of string tuples e.g. [(\"hi\", \"i\"), (\"i\", \"am\"), (\"am\", \"gabriele\")]\n",
    "        \"\"\"\n",
    "        regex = r'[a-zA-Z]+'  # r'\\w+' to include numbers as well\n",
    "        line_words = re.findall(regex, element.lower())  # clean punctuation: get a list of (re)\n",
    "        words_to_tuples = [(line_words[i], line_words[i+1]) for i in range(len(line_words)-1)]\n",
    "        return words_to_tuples\n",
    "\n",
    "\n",
    "\n",
    "class ExtractMostLikelyNextWordFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        \"\"\"\n",
    "        Pairs a word with its most likely successor.\n",
    "        element: a tuple, encoded as (word, [(next_word_i, count_i), ..., (next_word_n, count_n)])\n",
    "        return: the tuple (word, most_likely_next_word)\n",
    "        \"\"\"\n",
    "        word, next_list = element\n",
    "        next_list.sort(key=lambda wc_tuple: (-wc_tuple[1], wc_tuple[0]))\n",
    "        most_likely_successor = next_list[0][0]\n",
    "        return [(word, most_likely_successor)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computazioni leggere\n",
    "\n",
    "Basta definirle come semplici funzioni e passarle nella pipeline in una **lambda expression (Map, FlatMap, Reduce, ecc)**. Non verranno eseguite in parallelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_result(word_next_word):\n",
    "    \"\"\"\n",
    "    Converts to string a tuple (word, most_likely_successor)\n",
    "    word_next_word: tuple (word, most_likely_successor)\n",
    "    return: string \"word: most_likely_successor\"\n",
    "    \"\"\"\n",
    "    (word, most_likely_successor) = word_next_word\n",
    "    return \"{}: {}\".format(word, most_likely_successor)\n",
    "\n",
    "def count_ones(word_ones):\n",
    "    \"\"\"\n",
    "    Counts the ones associated with a (word, next) pair\n",
    "    word_ones: tuple (word, [1,1,..,1])\n",
    "    return: tuple (word, count)\n",
    "    \"\"\"\n",
    "    (word, ones) = word_ones\n",
    "    return (word, sum(ones))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 3.11439204216s\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE = 'alice.txt'\n",
    "OUTPUT_FILE = 'alice_processed.txt'\n",
    "\n",
    "start_time = time.time()\n",
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "    output = (p \n",
    "             | 'ReadInputFile'     >> beam.io.ReadFromText(INPUT_FILE)                # -> list lines\n",
    "             | 'CleanLines'        >> beam.ParDo(ComputeCleanLineFn())                # -> list tuple(word, next)\n",
    "             | 'WordNextWithOne'   >> beam.Map(lambda x: (x, 1))                      # -> tuple((word, next), 1)\n",
    "             | 'GroupByWordNext'   >> beam.GroupByKey()                               # -> tuple((word, next), [1,1,..1])\n",
    "             | 'WordNextWithCount' >> beam.Map(count_ones)                            # -> tuple((word, next), count))\n",
    "             | 'Reshape'           >> beam.Map(lambda x: (x[0][0], (x[0][1], x[1])))  # -> tuple(word, (next, count))\n",
    "             | 'GroupByWord'       >> beam.GroupByKey()                               # -> tuple(word, [(next_i, count_i), .., (next_n, count_n)])\n",
    "             | 'ChooseSuccessor'   >> beam.ParDo(ExtractMostLikelyNextWordFn())       # -> list tuple(word, next_k)\n",
    "             | 'FormatResult'      >> beam.Map(format_result)                         # string \"word: next_k\"\n",
    "             | 'WriteResult'       >> beam.io.WriteToText(OUTPUT_FILE)                # output file written\n",
    "            )\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()\n",
    "    \n",
    "print(\"elapsed time: {}s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipelines",
   "language": "python",
   "name": "pipelines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
